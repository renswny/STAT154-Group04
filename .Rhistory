predicted <- predict(fit_men, newdata=new)
pander(data.frame(new, predicted))
inches <- sapply(records$height, conversion)
inches
View(records)
set.seed(0)
library(e1071)
#(a):
n = 5000
p = 2
x1 = runif(n) - 0.5
x2 = runif(n) - 0.5
y = 1*( x1^2 - x2^2 > 0 )
df = data.frame( x1=x1, x2=x2, y=as.factor(y) )
#(b)
par(mar=c(4,3,3,1))
plot( x1, x2, col=(y+1), pch=19, cex=0.7,
xlab='x1', ylab='x2', main='initial data' )
#(c):
par(mfrow=c(1,2))
m = glm( y ~ x1 + x2, data=df, family=binomial )
# Part (d): Predict the class label of the training data.
y_hat = predict( m, newdata=data.frame(x1=x1,x2=x2), type="response" )
predicted_class = 1 * ( y_hat > 0.5 )
print(1 - sum( predicted_class == y )/length(y) ) #error rate
#plot
plot( x1, x2, col=(predicted_class+1), pch=19, cex=0.7, xlab='x1', ylab='x2',
main='logistic regression: y ~ x1 + x2' )
#(e-f): Using logistic regression to fit a nonlinear model:
#
m = glm( y ~ x1 + x2 + I(x1^2) + I(x2^2) + I(x1*x2), data=df, family="binomial" )
y_hat = predict( m, newdata=data.frame(x1=x1,x2=x2), type="response" )
predicted_class = 1 * ( y_hat > 0.5 )
print( 1 - sum( predicted_class == y )/length(y) ) #error rate
plot( x1, x2, col=(predicted_class+1),
pch=19, cex=0.7, xlab='x1', ylab='x2' )
tune.out = tune( svm, y ~ ., data=df, kernel="linear", ranges=list( cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000) ) )
install.packages("tm")
library(tm)
install.package("slam")
install.packages("slam")
library(tm)
install.packages('devtools')
library(devtools)
install_github('andreacirilloac/updateR')
install_github('andreacirilloac/updateR')
updateR(admin_password = 'A')
library(updateR)
updateR(admin_password = 'A')
install.packages("tm")
library(tm)
library(tm)
install.packages("slam")
emails <- read.table("data/HRC_train.tsv", sep="\t", header=FALSE)
setwd("/Users/Abigail/Documents/154FinalProject/")
library(tm)
emails <- read.table("data/HRC_train.tsv", sep="\t", header=FALSE)
View(emails)
emails[3,2]
typeof(emails[3,2])
typeof(emails[3,2][1])
typeof(emails[3,2][[1]]
)
getTransformations()
install.packages("dplyr")
e_1 <-emails[V1 == "1"]
e_1 <-emails[emails.V1 == "1"]
e_1 <-emails[emails$V1 == "1"]
e_1 <-emails[emails$V1 == "1", ]
nrow(e_1)
nrow(e)
nrow(emails)
hist(emails$V1)
emails <- read.table("data/HRC_train.tsv", sep="\t", header=FALSE, stringsAsFactors = FALSE)
coltypes(emails)
str(emails)
length(emails[1,2])
length(emails[1,4])
emails[3,2]
length(emails[3,2])
length(emails[3,2][[1]])
emails[3,2][[1]]
typeof(emails[3,2])
nchar(emails[3,2])
e_1 <-emails[emails$V1 == "1", ]
mean(nchar(e_1$V2))
summary(nchar(e_1$V2))
boxplot(nchar(emails$V2), emails$V1)
e_2 <-emails[emails$V2 == "2", ]
e_3 <-emails[emails$V3== "2", ]
e_3 <-emails[emails$V2== "3", ]
e_4 <-emails[emails$V2== "4", ]
e_5 <-emails[emails$V2== "5", ]
summary(nchar(e_2$V2))
e_2 <-emails[emails$V2 == "2", ]
summary(nchar(e_2$V2))
plot(nchar(emails$V2))
plot(nchar(emails$V2), emails$V1)
boxplot(nchar(emails$V2), emails$V1)
plot(nchar(emails$V2), emails$V1)
plot(mean(nchar(emails$V2)), emails$V1)
summary(nchar(e_3$V2))
summary(nchar(e_1$V2))
mean(nchar(e_3$V2))
head(nchar(e_3$V2))
e_2 <-emails[emails$V1 == "2", ]
e_3 <-emails[emails$V1== "3", ]
e_4 <-emails[emails$V1== "4", ]
e_5 <-emails[emails$V1== "5", ]
summary(nchar(e_2$V2))
summary(nchar(e_3$V2))
summary(nchar(e_4$V2))
emails$V1[4]
tm_map(emails$V2[3],content_transformer(tolower))
ex <- emails$V2[6]
ex
install.packages("stringr")
library(stringr)
strsplit(ex, " ")
tm_map(ex, content_transformer(tolower))
emails$wordvec[i] <-  strsplit(emails$V2[i])
for (i in 0:nrow(emails)) {
emails$wordvec[i] <-  strsplit(emails$V2[i])
}
for (i in 0:nrow(emails)) {
emails$wordvec[i] <-  strsplit(emails$V2[i], " ")
}
emailsC <- Corpus("data/HRC_train.tsv")
emailsC <- Corpus(DirSource("data/HRC_train.tsv"))
emailsC <- Corpus(DirSource("dataHRC_train.tsv"))
emailsC <- Corpus(DirSource("data"))
lower <- tmap(emailsC, content_transformer(tolower))
lower <- tmmap(emailsC, content_transformer(tolower))
lower <- tm_map(emailsC, content_transformer(tolower))
head(lower)
writeLines(as.character(emailsC[[30]]))
writeLines(as.character(emailsC[3]))
emailsC <- Corpus(VectorSource(emails$wordvec))
lower <- tm_map(emailsC, content_transformer(tolower))
head(lower)
str(emailsC)
View(emailsC)
View(lower)
writeLines(as.character(emailsC[[30]]))
writeLines(as.character(lower[[30]]))
length(emailsC)
nrow(emails)
emailsC[1]
emailsC[[1]]
processed <- tm_map(processed, removePunctuation)
processed <- tm_map(emailsC, content_transformer(tolower))
processed <- tm_map(processed, removePunctuation)
writeLines(as.character(processed[[30]]))
processed <- tm_map(processed, removeNumbers)
inspect(processed[5])
processed <- tm_map(processed, stemDocument)
library(SnowballC)
install.packages("SnowballC")
plot(nchar(emails$V2), emails$V1)
processed <- tm_map(processed, stemDocument)
writeLines(as.character(processed[[30]]))
processed <- tm_map(emailsC, content_transformer(tolower))
processed <- tm_map(processed, removePunctuation)
processed <- tm_map(processed, removeNumbers)
processed <- tm_map(processed, stemDocument, lazy=TRUE)
writeLines(as.character(processed[[30]]))
processed <- tm_map(processd, removeWords, stopwords(“english”))
processed <- tm_map(processd, removeWords, stopwords("english"))
processed <- tm_map(processed, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(processed)
dtm
freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing=TRUE)
freq[head(ord)]
freq[head(ord, n = 15)]
freq[tail(ord)]
freq[tail(ord, n = 20)]
common_string <- "UNCLASSIFIED U.S. Department of State Case No. F-2015-04841 Doc No. C05739597 Date: 05/13/2015 STATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM. SUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. RELEASE IN FULL"
for (i in 0:nrow(emails)) {
emails$V2 <- str_replace(emails$V2[i], common_string, "")
emails$wordvec[i] <-  strsplit(emails$V2[i], " ")
}
ex
str_replace(ex, common_string, "")
common_strings <- c("UNCLASSIFIED U.S. Department of State Case",
"PRODUCED TO HOUSE SELECT BENGHAZI COMM",
"SUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. RELEASE IN FULL")
ex_r <- str_replace(ex, common_strings, "")
ex_r
ex <- emails$V2[2]
ex
str_replace(ex, "UNCLASSIFIED U.S. Department of State Case", "")
freq[head(ord, n = 20)]
dtmr <-DocumentTermMatrix(docs, control=list(wordLengths=c(4, 20),
bounds = list(global = c(3,4000))))
dtmr <-DocumentTermMatrix(processed, control=list(wordLengths=c(4, 20), bounds = list(global = c(3,4000))))
dtmr
dtm
freq[head(ord, n = 20)]
freq[head(ord, n = 30)]
freq[head(ord, n = 50)]
freqr <- colSums(as.matrix(dtmr))
length(freqr)
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
dtmr <-DocumentTermMatrix(processed, control=list(wordLengths=c(4, 20), bounds = list(global = c(3,4000))))
dtmr
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
?DocumentTermMatrix
dtmr <-DocumentTermMatrix(processed, control=list(wordLengths=c(4, 20), bounds = list(c(3, 4000))))
freqr <- colSums(as.matrix(dtmr))
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
dtmr <- DocumentTermMatrix(processed, control=list(wordLengths=c(4, Inf), bounds = list(global= c(3, 4000))))
freqr <- colSums(as.matrix(dtmr))
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
dtmr
dtmt <- DocumentTermMatrix(processed, control = list(bounds = list(global=c(3, 40))))
dtmt
ordr <- order(freqr,decreasing=TRUE)
freqt <- colSums(as.matrix(dtmt))
ordt <- order(freqt, decreasing = TRUE)
freqt[head(ordt)]
dtmr <- DocumentTermMatrix(processed, control=list(bounds = list(global= c(3, 4000))))
freqr <- colSums(as.matrix(dtmr))
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
dtmr <- DocumentTermMatrix(processed, control=list(bounds = list(global= c(3, 1314))))
dtmr
freqr <- colSums(as.matrix(dtmr))
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
freqr[tail(ordr)]
dtmr <- DocumentTermMatrix(processed, control=list(bounds = list(global= c(3, 1300))))
freqr <- colSums(as.matrix(dtmr))
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
freqr[tail(ordr)]
dtmr <- DocumentTermMatrix(processed, control=list(wordLengths=c(4, Inf), bounds = list(global=c(3, 1300))))
freqr <- colSums(as.matrix(dtmr))
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
freqr[tail(ordr)]
typeof(dtmr)
dtmr
dtmr_1 <- dtmr[emails$V1 == "1", ]
dtmr_1
str(e_1)
dtmr_2 <- dtmr[emails$V1 == "2", ]
dtmr_3 <- dtmr[emails$V1 == "3", ]
dtmr_4 <- dtmr[emails$V1 == "4", ]
dtmr_5 <- dtmr[emails$V1 == "5", ]
dtmrs <- list()
freqs <- list()
for (i in 1:5) {
dtmrs[i] <- dtmr[emails$V1 == as.character(i), ]
freqs[i] <- colSums(as.matrix(dtmrs[i]))
}
for (i in 1:5) {
dtrmi <-  dtmr[emails$V1 == as.character(i), ]
assign(paste("dtmr",i, sep=""), dtrmi)
freqi <- colSums(as.matrix(dtrmi))
assign(paste("freq",i, sep=""), freqi)
}
head(freq3)
wordcloud(names(freq1), freq1, min.freq=100)
library(wordcloud)
install.packages("wordcloud")
install.packages("wordcloud")
dtmr <- DocumentTermMatrix(processed, control=list(bounds = list(global=c(3, 1300))))
wordcloud(names(freq1), freq1, min.freq=100)
library(wordcloud)
wordcloud(names(freq1), freq1, min.freq=100)
wordcloud(names(freq1), freq1, min.freq=1000)
wordcloud(names(freq1), freq1, min.freq=200)
wordcloud(names(freq2), freq2, min.freq=200)
wordcloud(names(freq2), freq2, min.freq=400)
for (i in 1:5) {
dtrmi <-  dtmr[emails$V1 == as.character(i), ]
assign(paste("dtmr",i, sep=""), dtrmi)
freqi <- colSums(as.matrix(dtrmi))
assign(paste("freq",i, sep=""), freqi)
}
dtmr
freqr <- colSums(as.matrix(dtmr))
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
dtmr2 <- DocumentTermMatrix(processed, control=list(bounds = list(global=c(3, 1200))))
freqr2 <- colSums(as.matrix(dtmr2))
ordr2 <- order(freqr2,decreasing=TRUE)
freqr2[head(ordr2)]
freqr <- colSums(as.matrix(dtmr))
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
freqr[tail(ordr)]
for (i in 1:5) {
dtrmi <-  dtmr[emails$V1 == as.character(i), ]
assign(paste("dtmr",i, sep=""), dtrmi)
freqi <- colSums(as.matrix(dtrmi))
assign(paste("freq",i, sep=""), freqi)
}
wordcloud(names(freq1), freq1, min.freq=200)
dtmr <- DocumentTermMatrix(processed, control=list(bounds = list(global=c(3, 1200))))
freqr <- colSums(as.matrix(dtmr))
ordr <- order(freqr,decreasing=TRUE)
freqr[head(ordr)]
freqr[tail(ordr)]
for (i in 1:5) {
dtrmi <-  dtmr[emails$V1 == as.character(i), ]
assign(paste("dtmr",i, sep=""), dtrmi)
freqi <- colSums(as.matrix(dtrmi))
assign(paste("freq",i, sep=""), freqi)
}
wordcloud(names(freq1), freq1, min.freq=200)
install.packages("ggplot2")
freq[head(ord, n = 20)]
freqr[head(ordr)]
df <- data.frame(dtmr)
df <- as.matrix(dtmr)
str(df)
m <- as.matrix(dtmr)
ex <- emails$wordvec[3]
mean(nchar(ex))
nchar(ex)
typeof(x)
typeof(ex)
ex
mean(length(ex))
length(ex)
ex <- as.vector(ex)
mean(nchar(ex))
nchar(ex[3])
nchar(ex)
typeof(ex)
ex <- as.vector(ex[1])
ex
ex <- as.vector(ex[[1]])
ex
nchar(ex)
as.vector((emails$wordvec[3])[[1]])
sum(nchar(vec))
sum(nchar(ex))
mean(nchar(vec))
mean_chars <- c()
total_chars <- c()
for (i in 0:nrow(emails)) {
vec <- as.vector((emails$wordvec[i])[[1]])
total_chars[i] <- sum(nchar(vec))
mean_chars[i] <- mean(nchar(vec))
}
for (i in 1:nrow(emails)) {
vec <- as.vector((emails$wordvec[i])[[1]])
total_chars[i] <- sum(nchar(vec))
mean_chars[i] <- mean(nchar(vec))
}
aov(total_chars ~ emails$V1)
summary(aov(total_chars ~ emails$V1))
summary(aov(mean_chars ~ emails$V1))
length(ex)
ex
for (i in 1:nrow(emails)) {
vec <- as.vector((emails$wordvec[i])[[1]])
total_chars[i] <- sum(nchar(vec))
mean_chars[i] <- mean(nchar(vec))
num_words[i] <- length(vec)
}
num_words <- c()
for (i in 1:nrow(emails)) {
vec <- as.vector((emails$wordvec[i])[[1]])
total_chars[i] <- sum(nchar(vec))
mean_chars[i] <- mean(nchar(vec))
num_words[i] <- length(vec)
}
summary(aov(num_words ~ emails$V1))
summary(aov(total_chars ~ emails$V1))
str_count(emails$V2[3], "\" )
)
)
str_count(emails$V2[3], pattern = "\n" )
str_count(emails$V2[3], pattern = "\" )
ç
s
str_count(emails$V2[3], pattern = "\&" )
str_count(emails$V2[3], pattern = "&" )
ampersands[i] <-str_count(emails$V2[3], pattern = "&" )
ampersands <- c()
str_count(emails$V2[3], pattern = "?" )
str_count(emails$V2[3], pattern = "\?" )
str_count(emails$V2[3], pattern = "/?" )
str_count(emails$V2[3], pattern = "/&" )
str_count(emails$V2[3], pattern = "&" )
str_count(emails$V2[3], pattern = "&" )
str_count(emails$V2[3], pattern = ";" )
str_count(emails$V2[3], pattern = "/;" )
str_count(emails$V2[10], pattern = "/;" )
str_count(emails$V2[10], pattern = ";" )
str_count(emails$V2[13], pattern = ";" )
str_count(emails$V2[13], pattern = """ )
)
)
str_count(emails$V2[13], pattern = " "*" " )
str_count(emails$V2[13], pattern = " /" " )
str_count(emails$V2[13], pattern = " /"([a-z,0-9]*)/"" " )
ampersands <- c()
qmarks <- c()
semicolons <- c()
for (i in 1:nrow(emails)) {
vec <- as.vector((emails$wordvec[i])[[1]])
total_chars[i] <- sum(nchar(vec))
mean_chars[i] <- mean(nchar(vec))
num_words[i] <- length(vec)
ampersands[i] <-str_count(emails$V2[i], pattern = "&" )
qmarks[i] <-str_count(emails$V2[i], pattern = "/?" )
semicolons[i] <-str_count(emails$V2[i], pattern = ";" )
}
summary(aov(ampersands ~ emails$V1))
summary(aov(qmarks ~ emails$V1))
qmarks_per_word <- c()
semicolons_per_word <- c()
for (i in 1:nrow(emails)) {
vec <- as.vector((emails$wordvec[i])[[1]])
total_chars[i] <- sum(nchar(vec))
mean_chars[i] <- mean(nchar(vec))
num_words[i] <- length(vec)
ampersands[i] <-str_count(emails$V2[i], pattern = "&" )
qmarks[i] <-str_count(emails$V2[i], pattern = "/?" )
qmarks_per_word[i] <- qmarks[i]/numwords[i]
semicolons[i] <-str_count(emails$V2[i], pattern = ";" )
semicolons_per_word[i] <- qmarks[i]/numwords[i]
}
qmarks_per_word <- c()
semicolons_per_word <- c()
for (i in 1:nrow(emails)) {
vec <- as.vector((emails$wordvec[i])[[1]])
total_chars[i] <- sum(nchar(vec))
mean_chars[i] <- mean(nchar(vec))
num_words[i] <- length(vec)
ampersands[i] <-str_count(emails$V2[i], pattern = "&" )
qmarks[i] <-str_count(emails$V2[i], pattern = "/?" )
qmarks_per_word[i] <- qmarks[i]/num_words[i]
semicolons[i] <-str_count(emails$V2[i], pattern = ";" )
semicolons_per_word[i] <- qmarks[i]/num_words[i]
}
summary(aov(qmarks_per_word ~ emails$V1))
summary(aov(semicolons_per_word ~ emails$V1))
total_chars <- c()
mean_chars <- c()
num_words <- c()
ampersands <- c()
qmarks <- c()
semicolons <- c()
qmarks_per_word <- c()
semicolons_per_word <- c()
for (i in 1:nrow(emails)) {
vec <- as.vector((emails$wordvec[i])[[1]])
total_chars[i] <- sum(nchar(vec))
mean_chars[i] <- mean(nchar(vec))
num_words[i] <- length(vec)
ampersands[i] <-str_count(emails$V2[i], pattern = "&" )
qmarks[i] <-str_count(emails$V2[i], pattern = "/?" )
qmarks_per_word[i] <- qmarks[i]/num_words[i]
semicolons[i] <-str_count(emails$V2[i], pattern = ";" )
semicolons_per_word[i] <- semicolons[i]/num_words[i]
}
summary(aov(semicolons_per_word ~ emails$V1))
str_count(emails$V2[3], pattern = "[A-Z]" )
uppercase_per_word <- c()
for (i in 1:nrow(emails)) {
vec <- as.vector((emails$wordvec[i])[[1]])
total_chars[i] <- sum(nchar(vec))
mean_chars[i] <- mean(nchar(vec))
num_words[i] <- length(vec)
ampersands[i] <-str_count(emails$V2[i], pattern = "&" )
qmarks[i] <-str_count(emails$V2[i], pattern = "/?" )
qmarks_per_word[i] <- qmarks[i]/num_words[i]
semicolons[i] <-str_count(emails$V2[i], pattern = ";" )
semicolons_per_word[i] <- semicolons[i]/num_words[i]
uppercase_per_word <- str_count(emails$V2[i], pattern = "[A-Z]" )/num_words[i]
}
summary(aov(uppercase_per_word ~ emails$V1))
for (i in 1:nrow(emails)) {
uppercase_per_word[i] <- str_count(emails$V2[i], pattern = "[A-Z]" )/num_words[i]
}
summary(aov(uppercase_per_word ~ emails$V1))
df <- data.frame(emails$V1, numwords, m)
df <- data.frame(emails$V1, numw_ords, m)
df <- data.frame(emails$V1, num_words, m)
write.csv("data/processed_df.csv", df)
?write.csv
write.csv(df, file="data/processed_df.csv")
s <- sample(c(1:1315), 1315/4)
s
test <- df[s,]
train <- df[-s, ]
str(train)
write.csv(test, file="data/test_df.csv")
write.csv(test, file="data/train_df.csv")
?tm_map
summary(aov(num_words ~ emails$V1))
emailsC
processed <- tm_map(emailsC, content_transformer(tolower))
processed <- tm_map(processed, removePunctuation)
processed <- tm_map(processed, removeNumbers)
dtm_raw <- DocumentTermMatrix(processed)
dtm_raw
processed <- tm_map(processed, removeWords, stopwords("english"))
dtm_stop <- DocumentTermMatrix(processed)
dtm_stop
processed <- tm_map(processed, stemDocument, lazy=TRUE)
dtm_stem <- DocumentTermMatrix(processed)
