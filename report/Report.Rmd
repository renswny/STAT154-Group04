---
title: "Classifing HRC's Emails by Sender"
author: "Pranay Singal, Kensen Tan, Renee Sweeney, Abigail Chaver"
date: "December 2, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this project, our goal is to classify the sender of emails found on Hillary Clinton's email server. We will use a word frequency table as our primary source of predictors, and will use 3 modeling approaches: Random Forest, Support Vector Machine, and K-means Clustering. We will compare the results of our supervised methods to select our final model.

```{r, include=FALSE}
library(pander)
```

## Data Processing

The initial data consisted of a tab separated file with two columns: the sender code (labeled 1-5) and the string of the email.

After reading in the tsv, we converted the string to a character vector. We then wrote those vectors to a corpus so that we could use the R package `tm` to remove punctuation, numbers, and stop words; convert to lower case; and stem. 

Next, we removed common words that were seen in every email, such as "subject", and "US". 

We also explored other possible features that could predict sender, including number of words, average word length, and rate of use for ampersands, semicolons, question marks, and upper case. We found that the only variable which differed significantly between groups was number of words per email, so we added that as a feature to our matrix.

```{r, echo=FALSE}
Terms <- c(38066, 37966, 27504, 9177, 9178)
Steps <- c("Raw", "Stop Words", "Stemmed", "Frequency Selection", "Additional Features")
pander(data.frame(Steps, Terms))
```




```{r, include=FALSE}
library(pander)
```

# Random Forest Model

Explanation...

```{r echo=FALSE}
Step <- c("RF", "RF with Feature Selection")
Features_Used <- c(0, 0)
Total_Accuracy <- c(0, 0)
Accuracy_per_class <- c(0, 0)
df <- data.frame(Step, Features_Used, Total_Accuracy, Accuracy_per_class)
colnames(df) <- c("Step", " Number of Features Used", "Accuracy", "Accuracy by Class")
pander(df)
```


## Top Ten Features
```{r echo=FALSE}
Rank <- c(1:10)
Feature <- c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j")
pander(data.frame(Rank, Feature))
```


```{r, include=FALSE}
library(pander)
```

# SVM Model

Explanation...


```{r echo=FALSE}
Step <- c("SVM", "SVM with Feature Selection")
Features_Used <- c(0, 0)
Total_Accuracy <- c(0, 0)
Accuracy_per_class <- c(0, 0)
df <- data.frame(Step, Features_Used, Total_Accuracy, Accuracy_per_class)
colnames(df) <- c("Step", " Number of Features Used", "Accuracy", "Accuracy by Class")
pander(df)
```

## Top Ten Features
```{r echo=FALSE}
Rank <- c(1:10)
Feature <- c("a", "b", "c", "d", "e", "f", "g", "h", "i", "j")
pander(data.frame(Rank, Feature))
```


# K-means Clustering



# Model Selection

Compare MSEs here.


# Predict Classifications on Test Set

Using our best model, we used `predict` to generate classifications for each email in the test set.
# Conclusion